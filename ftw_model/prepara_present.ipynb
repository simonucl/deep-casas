{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from train import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "args = {'model': 'BiLSTM', 'features': '../hh_dataset/hh_npy/fib_hh102_feature.npy',\n",
    "        'activities': '../hh_dataset/hh_npy/fib_hh102_activity.npy', \n",
    "        'feature_encoding': '1d_cnn', \n",
    "        'delta': 10}\n",
    "args = [f\"--{k}={v}\" for k, v in args.items()]\n",
    "p = argparse.ArgumentParser()\n",
    "p.add_argument('--model', dest='model', action='store', default='', help='deep model')\n",
    "p.add_argument('--features', action='store', default='', required=True, help='deep model')\n",
    "p.add_argument('--activities', action='store', default='', required=True, help='deep model')\n",
    "p.add_argument('--feature_encoding', action='store', default='mean_with_weight', required=True, help='deep model')\n",
    "p.add_argument('--delta', action='store', default='20', required=False, help='deep model')\n",
    "args = p.parse_args(args)\n",
    "\n",
    "models = main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models1 = deepcopy(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(pred, target, threshold=0.3):\n",
    "    pred = np.array(pred > threshold, dtype=float)\n",
    "    return {'micro/precision': precision_score(y_true=target, y_pred=pred, average='micro'),\n",
    "            'micro/recall': recall_score(y_true=target, y_pred=pred, average='micro'),\n",
    "            'micro/f1': f1_score(y_true=target, y_pred=pred, average='micro'),\n",
    "            'macro/precision': precision_score(y_true=target, y_pred=pred, average='macro'),\n",
    "            'macro/recall': recall_score(y_true=target, y_pred=pred, average='macro'),\n",
    "            'macro/f1': f1_score(y_true=target, y_pred=pred, average='macro'),\n",
    "            'samples/precision': precision_score(y_true=target, y_pred=pred, average='samples'),\n",
    "            'samples/recall': recall_score(y_true=target, y_pred=pred, average='samples'),\n",
    "            'samples/f1': f1_score(y_true=target, y_pred=pred, average='samples'),\n",
    "            }\n",
    "\n",
    "X_train, X_test, y_train, y_test = processed_features[train], processed_features[test], activities[train], activities[test]\n",
    "\n",
    "batch_size = 32\n",
    "n_iters = 100\n",
    "\n",
    "train_features, train_labels = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
    "dev_features, dev_labels = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "train_dataset = DataLoader(ActivityDataset(train_features, train_labels), batch_size=batch_size, shuffle=False)\n",
    "dev_dataset = DataLoader(ActivityDataset(dev_features, dev_labels), batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "for features, labels in tqdm(dev_dataset, total=len(dev_dataset)):\n",
    "# train_labels, train_tensor = torch.tensor(train_activities, dtype=torch.float32), torch.tensor(train_features, dtype=torch.float32)\n",
    "# dev_labels, dev_tensor = torch.tensor(dev_activities, dtype=torch.float32), torch.tensor(dev_features, dtype=torch.float32)\n",
    "\n",
    "    features = features.to(device).float()\n",
    "    labels = labels.to(device).float()\n",
    "\n",
    "    outputs = torch.Tensor([])\n",
    "    for model in models:\n",
    "        output, logits = model(features)\n",
    "        # print(output.shape)\n",
    "        outputs = torch.cat((outputs, output.detach().cpu()), dim=-1)\n",
    "\n",
    "    # print(outputs.shape)\n",
    "    # print(labels.shape)\n",
    "    pred = outputs.numpy().round()\n",
    "    result = calculate_metrics(pred, labels.cpu().numpy())\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_train(X_train, X_test, y_train, y_test, models):\n",
    "    batch_size = 16\n",
    "    n_iters = 100\n",
    "    total_classes = np.sum(y_train.T, axis=1)\n",
    "    n_categories = len(y_train.T) - 1\n",
    "    \n",
    "    medians = [total_classes[k]/len(y_train) for k in range(0,n_categories)]\n",
    "    median_all = np.mean(medians)\n",
    "    my_freqs = median_all/(np.float64(y_train)+(10E-14))\n",
    "\n",
    "    model = Multi(models, len(models), len(models))\n",
    "    learning_rate = 0.0001\n",
    "    optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    criterion = FocalLoss(torch.tensor(my_freqs).to(device))\n",
    "\n",
    "    train_features, train_labels = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
    "    dev_features, dev_labels = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
    "    \n",
    "    for iter in trange(1, n_iters + 1):\n",
    "        train_dataset = DataLoader(ActivityDataset(train_features, train_labels), batch_size=batch_size, shuffle=False)\n",
    "        dev_dataset = DataLoader(ActivityDataset(dev_features, dev_labels), batch_size=len(y_test), shuffle=False)\n",
    "        model.train()\n",
    "        training_loss = 0\n",
    "        best_f1_score = 0\n",
    "        best_classification_report = None\n",
    "\n",
    "        for features, labels in tqdm(train_dataset, total=len(train_dataset)):\n",
    "        # train_labels, train_tensor = torch.tensor(train_activities, dtype=torch.float32), torch.tensor(train_features, dtype=torch.float32)\n",
    "        # dev_labels, dev_tensor = torch.tensor(dev_activities, dtype=torch.float32), torch.tensor(dev_features, dtype=torch.float32)\n",
    "\n",
    "            features = features.to(device).float()\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            output, logits = model(features)\n",
    "            # loss = criterion(logits, labels)\n",
    "            pred = logits.cpu().detach().numpy().round()\n",
    "            result = calculate_metrics(pred, labels.cpu().numpy())\n",
    "\n",
    "            # loss.backward()\n",
    "            # optimizer.step()\n",
    "            # optimizer.zero_grad()\n",
    "            \n",
    "            validation_loss = None\n",
    "            #scheduler.step()\n",
    "            current_loss = 0\n",
    "            \n",
    "        for dev_tensor, dev_activities in dev_dataset:\n",
    "            dev_tensor = dev_tensor.to(device)\n",
    "            dev_labels = dev_activities.to(device)\n",
    "            if len(dev_labels.shape) < 2:\n",
    "                dev_labels = dev_labels.unsqueeze(1)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                prediction, logits = model(dev_tensor)\n",
    "                \n",
    "                pred = logits.cpu().detach().numpy().round()\n",
    "\n",
    "                # validation_loss = criterion(logits, dev_labels)\n",
    "                result = calculate_metrics(pred, dev_activities.cpu().numpy())\n",
    "                \n",
    "                #backprop\n",
    "                if result['micro/f1'] > best_f1_score:\n",
    "                    best_f1_score = result['micro/f1']\n",
    "                    best_classification_report = result\n",
    "                    best_epoch = iter\n",
    "                    best_validation_loss = validation_loss\n",
    "                validation_loss = 0\n",
    "                \n",
    "            if iter%print_every == 0:\n",
    "                accur.append(result)\n",
    "                \n",
    "                # result_file.write(\"epoch {}\\tloss : {}\\t accuracy : {}\\n\".format(iter,loss,acc))\n",
    "                print(\"epoch {}\\tloss : {}\\t accuracy : {}\".format(iter,validation_loss,result))\n",
    "                # print(classification_report(dev_activities, pred, digits=4))\n",
    "        \n",
    "        # current_loss += validation_loss.item()\n",
    "        # if early_stopper.early_stop(validation_loss):\n",
    "        #     print(\"epoch {}\\tloss : {}\\t accuracy : {}\".format(best_epoch,best_validation_loss,best_classification_report))\n",
    "        #     # result_file.write(\"Best epoch at \" + str(best_epoch) + \". Early stopping at epoch \" + str(iter))\n",
    "        #     break\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            all_losses.append(current_loss / plot_every)\n",
    "            current_loss = 0\n",
    "        # result_file.write(str(best_classification_report) + '\\n')\n",
    "        # return pd.DataFrame(best_classification_report).round(4)\n",
    "        \n",
    "        # ensemble_model_results = multi_label_train(X_train, X_test, y_train, y_test, models)\n",
    "multi_label_train(X_train, X_test, y_train, y_test, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 419/419 [00:08<00:00, 51.75it/s]\n",
      "  1%|          | 1/100 [00:09<15:35,  9.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\tloss : 0.015425018034875393\t accuracy : {'micro/precision': 0.8050036791758646, 'micro/recall': 0.812778603268945, 'micro/f1': 0.8088724584103512, 'macro/precision': 0.774747322385416, 'macro/recall': 0.7950185878400265, 'macro/f1': 0.7801323493842599, 'samples/precision': 0.334453907067085, 'samples/recall': 0.33687434633198865, 'samples/f1': 0.3302572694997616}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 419/419 [00:08<00:00, 51.67it/s]\n",
      "  2%|▏         | 2/100 [00:18<15:26,  9.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2\tloss : 0.015425018034875393\t accuracy : {'micro/precision': 0.8050036791758646, 'micro/recall': 0.812778603268945, 'micro/f1': 0.8088724584103512, 'macro/precision': 0.774747322385416, 'macro/recall': 0.7950185878400265, 'macro/f1': 0.7801323493842599, 'samples/precision': 0.334453907067085, 'samples/recall': 0.33687434633198865, 'samples/f1': 0.3302572694997616}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 145/419 [00:02<00:05, 50.71it/s]\n",
      "  2%|▏         | 2/100 [00:21<17:46, 10.88s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3315441/3961672038.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# ensemble_model_results = multi_label_train(X_train, X_test, y_train, y_test, models)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m \u001b[0mmulti_label_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3315441/3961672038.py\u001b[0m in \u001b[0;36mmulti_label_train\u001b[0;34m(X_train, X_test, y_train, y_test, models)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/casas/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3315441/3961672038.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;31m# print(output.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/casas/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep-casas/ftw_model/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mfeature_1d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_1d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# print(out.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/casas/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/casas/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 770\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    771\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from copy import deepcopy\n",
    "import tqdm\n",
    "\n",
    "from numpy.typing import NDArray\n",
    "from over_sample import get_minority_instace, MLSMOTE\n",
    "from model import LSTM, EarlyStopper, LSTM_1d, Multi_out_LSTM, FocalLoss\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit, train_test_split\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "device = \"cuda:0\"\n",
    "class Multi(nn.Module):\n",
    "\n",
    "    def __init__(self, models, input_dim,output_dim):\n",
    "        super(Multi, self).__init__()\n",
    "        self.models = models\n",
    "        self.out = nn.Sequential(\n",
    "            # nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=input_dim, out_features=output_dim)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = torch.Tensor([]).to(device)\n",
    "        for model in models:\n",
    "            model = model.to(device)\n",
    "            output, logits = model(inputs)\n",
    "            # print(output.shape)\n",
    "            outputs = torch.cat((outputs, logits), dim=-1)\n",
    "        # self.out = self.out.to(device)\n",
    "        # logits = self.out(outputs)\n",
    "        return self.sigmoid(outputs), outputs\n",
    "    \n",
    "def calculate_metrics(pred, target, threshold=0.3):\n",
    "    pred = np.array(pred > threshold, dtype=float)\n",
    "    return {'micro/precision': precision_score(y_true=target, y_pred=pred, average='micro'),\n",
    "            'micro/recall': recall_score(y_true=target, y_pred=pred, average='micro'),\n",
    "            'micro/f1': f1_score(y_true=target, y_pred=pred, average='micro'),\n",
    "            'macro/precision': precision_score(y_true=target, y_pred=pred, average='macro'),\n",
    "            'macro/recall': recall_score(y_true=target, y_pred=pred, average='macro'),\n",
    "            'macro/f1': f1_score(y_true=target, y_pred=pred, average='macro'),\n",
    "            'samples/precision': precision_score(y_true=target, y_pred=pred, average='samples'),\n",
    "            'samples/recall': recall_score(y_true=target, y_pred=pred, average='samples'),\n",
    "            'samples/f1': f1_score(y_true=target, y_pred=pred, average='samples'),\n",
    "            }\n",
    "\n",
    "class ActivityDataset(Dataset):\n",
    "  \"\"\"\n",
    "  This is a custom dataset class. It can get more complex than this, but simplified so you can understand what's happening here without\n",
    "  getting bogged down by the preprocessing\n",
    "  \"\"\"\n",
    "  def __init__(self, X, Y):\n",
    "    self.X = X\n",
    "    self.Y = Y\n",
    "    if len(self.X) != len(self.Y):\n",
    "      raise Exception(\"The length of X does not match the length of Y\")\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # note that this isn't randomly selecting. It's a simple get a single item that represents an x and y\n",
    "    _x = self.X[index]\n",
    "    _y = self.Y[index]\n",
    "\n",
    "    return _x, _y\n",
    "  \n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "features = np.load(args.features)\n",
    "activities = np.load(args.activities)\n",
    "\n",
    "mapping_file = args.features.rsplit('_', 1)[0] + '_mapping.json'\n",
    "with open(mapping_file, 'r') as f:\n",
    "    activitiy_mapping = json.load(f)\n",
    "ensemble_activities = activities.T\n",
    "\n",
    "processed_features = features\n",
    "\n",
    "input_size = processed_features.shape[-1]\n",
    "\n",
    "n_hidden = 512\n",
    "n_categories = len(activitiy_mapping) - 1\n",
    "n_layer = 3\n",
    "\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10000, gamma=0.1)\n",
    "\n",
    "n_iters = 1000\n",
    "print_every = n_iters // 200\n",
    "plot_every =  n_iters // 200\n",
    "batch_size = 32\n",
    "\n",
    "print_every = 1\n",
    "tsv = TimeSeriesSplit(n_splits=3)\n",
    "kfold = KFold(n_splits=3)\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "accur = []\n",
    "train, test = list(tsv.split(processed_features, activities))[-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = processed_features[train], processed_features[test], activities[train], activities[test]\n",
    "\n",
    "def multi_label_train(X_train, X_test, y_train, y_test, models):\n",
    "    batch_size = 16\n",
    "    n_iters = 100\n",
    "    total_classes = np.sum(activities.T, axis=1)\n",
    "\n",
    "    medians = [total_classes[k]/len(activities) for k in range(0,n_categories)]\n",
    "    median_all = np.mean(medians)\n",
    "    my_freqs = median_all/(np.float64(medians)+(10E-14))\n",
    "\n",
    "    model = Multi(models, len(models), len(models))\n",
    "    learning_rate = 0.0001\n",
    "    optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    criterion = FocalLoss(torch.tensor(my_freqs).to(device))\n",
    "\n",
    "    train_features, train_labels = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
    "    dev_features, dev_labels = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
    "    \n",
    "    for iter in trange(1, n_iters + 1):\n",
    "        train_dataset = DataLoader(ActivityDataset(train_features, train_labels), batch_size=batch_size, shuffle=False)\n",
    "        dev_dataset = DataLoader(ActivityDataset(dev_features, dev_labels), batch_size=len(y_test), shuffle=False)\n",
    "        model.train()\n",
    "        training_loss = 0\n",
    "        best_f1_score = 0\n",
    "        best_classification_report = None\n",
    "\n",
    "        for features, labels in tqdm(train_dataset, total=len(train_dataset)):\n",
    "        # train_labels, train_tensor = torch.tensor(train_activities, dtype=torch.float32), torch.tensor(train_features, dtype=torch.float32)\n",
    "        # dev_labels, dev_tensor = torch.tensor(dev_activities, dtype=torch.float32), torch.tensor(dev_features, dtype=torch.float32)\n",
    "\n",
    "            features = features.to(device).float()\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            output, logits = model(features)\n",
    "            loss = criterion(logits, labels)\n",
    "            pred = output.cpu().detach().numpy().round()\n",
    "            result = calculate_metrics(pred, labels.cpu().numpy())\n",
    "\n",
    "            # loss.backward()\n",
    "            # optimizer.step()\n",
    "            # optimizer.zero_grad()\n",
    "            \n",
    "            validation_loss = None\n",
    "            #scheduler.step()\n",
    "            current_loss = 0\n",
    "            \n",
    "        for dev_tensor, dev_activities in dev_dataset:\n",
    "            dev_tensor = dev_tensor.to(device)\n",
    "            dev_labels = dev_activities.to(device)\n",
    "            if len(dev_labels.shape) < 2:\n",
    "                dev_labels = dev_labels.unsqueeze(1)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                prediction, logits = model(dev_tensor)\n",
    "                \n",
    "                pred = prediction.cpu().detach().numpy().round()\n",
    "\n",
    "                validation_loss = criterion(logits, dev_labels)\n",
    "                result = calculate_metrics(pred, dev_activities.cpu().numpy())\n",
    "                \n",
    "                #backprop\n",
    "                if result['micro/f1'] > best_f1_score:\n",
    "                    best_f1_score = result['micro/f1']\n",
    "                    best_classification_report = result\n",
    "                    best_epoch = iter\n",
    "                    best_validation_loss = validation_loss\n",
    "\n",
    "            if iter%print_every == 0:\n",
    "                accur.append(result)\n",
    "                \n",
    "                # result_file.write(\"epoch {}\\tloss : {}\\t accuracy : {}\\n\".format(iter,loss,acc))\n",
    "                print(\"epoch {}\\tloss : {}\\t accuracy : {}\".format(iter,validation_loss,result))\n",
    "                # print(classification_report(dev_activities, pred, digits=4))\n",
    "        \n",
    "        current_loss += validation_loss.item()\n",
    "        # if early_stopper.early_stop(validation_loss):\n",
    "        #     print(\"epoch {}\\tloss : {}\\t accuracy : {}\".format(best_epoch,best_validation_loss,best_classification_report))\n",
    "        #     # result_file.write(\"Best epoch at \" + str(best_epoch) + \". Early stopping at epoch \" + str(iter))\n",
    "        #     break\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            all_losses.append(current_loss / plot_every)\n",
    "            current_loss = 0\n",
    "        # result_file.write(str(best_classification_report) + '\\n')\n",
    "        # return pd.DataFrame(best_classification_report).round(4)\n",
    "        \n",
    "        # ensemble_model_results = multi_label_train(X_train, X_test, y_train, y_test, models)\n",
    "multi_label_train(X_train, X_test, y_train, y_test, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2file = {'original': 'BiLSTM-original_feature.tsv', \n",
    "'Fib_FTW': 'BiLSTM-fib_ftw.tsv', \n",
    "'mean_feature':'BiLSTM-22-01-23-18:42:59 (mean).tsv',\n",
    "'mean_std':'BiLSTM-22-01-23-19:05:49(mean_std).tsv',\n",
    "'mean_with_weight':'BiLSTM-22-01-23-22:07:31(mean_with_weight).tsv',\n",
    "'10mins_feature': 'BiLSTM-22-01-23-22:24:13(10_mins_features).tsv',\n",
    "'5mins_feature':'BiLSTM-23-01-23-01:18:51(5_mins_features).tsv',\n",
    "'fib_ftw_feature':'BiLSTM-23-01-23-02:51:59(fib_ftw_features).tsv',\n",
    "'fib_ftw_5mins_feature':'BiLSTM-fib-ftw-5mins.tsv',\n",
    "'fib_ftw_2mins_feature':'BiLSTM-fib-ftw-2mins.tsv'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "activity_pos_detail = {}\n",
    "activity_marco_detail = {}\n",
    "overall = defaultdict(dict)\n",
    "present_results = ['mean_with_weight', 'fib_ftw_feature', 'fib_ftw_5mins_feature', 'fib_ftw_2mins_feature']\n",
    "for result in present_results:\n",
    "    csv = pd.read_csv('./result/' + result2file[result], sep='\\t').set_index('activity')\n",
    "    activity_pos_detail[result] = csv['positive_f1']\n",
    "    activity_marco_detail[result] = csv['marco_f1']\n",
    "    if 'Mean' not in csv.index:\n",
    "        overall[result]['positive_f1'] = csv['positive_f1'].astype(float).mean(axis=0)\n",
    "        overall[result]['marco_f1'] = csv['marco_f1'].astype(float).mean(axis=0)\n",
    "    else:\n",
    "        for col, average in csv[['positive_f1', 'marco_f1']].loc['Mean'].items():\n",
    "            overall[result][col] = average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(overall).T.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(overall).T.to_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>mean_feature</th>\n",
    "      <th>mean_std</th>\n",
    "      <th>mean_with_weight</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>positive_f1</th>\n",
    "      <td>0.6020</td>\n",
    "      <td>0.5580</td>\n",
    "      <td>0.6533</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>marco_f1</th>\n",
    "      <td>0.7911</td>\n",
    "      <td>0.7669</td>\n",
    "      <td>0.8169</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "features = {}\n",
    "activities = {}\n",
    "\n",
    "for dir, _, files in list(os.walk('../hh_dataset/hh_npy')):\n",
    "    for file in files:\n",
    "        # print(file)\n",
    "        npy_file = np.load(os.path.join(dir, file))\n",
    "        if 'feature' in file:\n",
    "            features[file] = npy_file.shape\n",
    "        else:\n",
    "            activities[file] = npy_file.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features)\n",
    "# print(activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "lamd = 0.3\n",
    "denom = [np.exp(-lamd * i) for i in range(10)]\n",
    "denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pred = np.load('./result_new/BiLSTM-(fib_hh102)_mean_with_weight/predictions.npy', allow_pickle=True)\n",
    "act = np.load('./result_new/BiLSTM-(fib_hh102)_mean_with_weight/activities.npy', allow_pickle=True)\n",
    "over_sample_pred = np.load('./result_new/BiLSTM-(fib_hh102)_mean_with_weight_oversample/predictions.npy', allow_pickle=True)\n",
    "over_sample_act = np.load('./result_new/BiLSTM-(fib_hh102)_mean_with_weight_oversample/activities.npy', allow_pickle=True)\n",
    "cnn_pred = np.load('./result_new/BiLSTM-(fib_hh102)_1d_cnn_oversample/predictions.npy', allow_pickle=True)\n",
    "cnn_act = np.load('./result_new/BiLSTM-(fib_hh102)_1d_cnn_oversample/activities.npy', allow_pickle=True)\n",
    "\n",
    "cnn_pred = np.load('./result_new/BiLSTM-(fib_hh102)_mean_with_weightwithout_act_mapping/predictions.npy', allow_pickle=True)\n",
    "cnn_act = np.load('./result_new/BiLSTM-(fib_hh102)_mean_with_weightwithout_act_mapping/activities.npy', allow_pickle=True)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act[-1115:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activitiy_mapping = {'Sleep': 0,\n",
    "    'Bed_Toilet_Transition': 1,\n",
    "    'Toilet': 2,\n",
    "    'Take_Medicine': 3,\n",
    "    'Dress': 4,\n",
    "    'Work': 5,\n",
    "    'Cook': 6,\n",
    "    'Eat': 7,\n",
    "    'Wash_Dishes': 8,\n",
    "    'Relax': 9,\n",
    "    'Personal_Hygiene': 10,\n",
    "    'Bathe': 11,\n",
    "    'Groom': 12,\n",
    "    'Drink': 13,\n",
    "    'Leave_Home': 14,\n",
    "    'Enter_Home': 15,\n",
    "    'Phone': 16,\n",
    "    'Other_Activity': 17}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "def calculate_metrics(pred, target, threshold=0.0):\n",
    "    pred = np.array(pred > threshold, dtype=float)\n",
    "    return {'micro/precision': precision_score(y_true=target, y_pred=pred, average='micro'),\n",
    "            'micro/recall': recall_score(y_true=target, y_pred=pred, average='micro'),\n",
    "            'micro/f1': f1_score(y_true=target, y_pred=pred, average='micro'),\n",
    "            'macro/precision': precision_score(y_true=target, y_pred=pred, average='macro'),\n",
    "            'macro/recall': recall_score(y_true=target, y_pred=pred, average='macro'),\n",
    "            'macro/f1': f1_score(y_true=target, y_pred=pred, average='macro'),\n",
    "            'samples/precision': precision_score(y_true=target, y_pred=pred, average='weighted'),\n",
    "            'samples/recall': recall_score(y_true=target, y_pred=pred, average='weighted'),\n",
    "            'samples/f1': f1_score(y_true=target, y_pred=pred, average='weighted'),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(pred.T, act[-1115:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def t2v(tau, f, out_features, w, b, w0, b0, arg=None):\n",
    "    if arg:\n",
    "        v1 = f(torch.matmul(tau, w) + b, arg)\n",
    "    else:\n",
    "        #print(w.shape, t1.shape, b.shape)\n",
    "        v1 = f(torch.matmul(tau, w) + b)\n",
    "    v2 = torch.matmul(tau, w0) + b0\n",
    "    #print(v1.shape)\n",
    "    return torch.cat([v1, v2], -1)\n",
    "\n",
    "class SineActivation(nn.Module):\n",
    "    def __init__(self, batch_size, in_features, out_features):\n",
    "        super(SineActivation, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.w0 = nn.parameter.Parameter(torch.randn(in_features, 1))\n",
    "        self.b0 = nn.parameter.Parameter(torch.randn(1))\n",
    "        self.w = nn.parameter.Parameter(torch.randn(in_features, out_features-1))\n",
    "        # torch.Tensor.fill_(self.w, torch.pi * 2 / 7)\n",
    "        # torch.Tensor.fill_(self.w0, torch.pi * 2 / 7)\n",
    "        self.b = nn.parameter.Parameter(torch.randn(out_features-1))\n",
    "        self.f = torch.sin\n",
    "\n",
    "    def forward(self, tau):\n",
    "        # tau.shape = (1, in_features)\n",
    "        # w.shape = (in_features, out_features-1)\n",
    "        # w0.shape = (in_features, 1)\n",
    "        return t2v(tau, self.f, self.out_features, self.w, self.b, self.w0, self.b0)\n",
    "\n",
    "class CosineActivation(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(CosineActivation, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.w0 = nn.parameter.Parameter(torch.randn(in_features, 1))\n",
    "        self.b0 = nn.parameter.Parameter(torch.randn(1))\n",
    "        self.w = nn.parameter.Parameter(torch.randn(in_features, out_features-1))\n",
    "        self.b = nn.parameter.Parameter(torch.randn(out_features-1))\n",
    "        self.f = torch.cos\n",
    "\n",
    "    def forward(self, tau):\n",
    "        return t2v(tau, self.f, self.out_features, self.w, self.b, self.w0, self.b0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 56])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "out_features = 56\n",
    "sineact = SineActivation(batch_size, 1, out_features)\n",
    "# cosact = CosineActivation(1, 64)\n",
    "\n",
    "# print(sineact(torch.Tensor([[5, 10]])).shape)\n",
    "a = torch.rand(batch_size, out_features)\n",
    "time = torch.tensor([[1, 1]], dtype=torch.float).reshape(-1, 1)\n",
    "print(sineact(torch.Tensor(time)).shape)\n",
    "# # np.array([[5, 10]]).shape\n",
    "sineact(torch.Tensor(time))\n",
    "time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8976, 0.8976, 0.8976, 0.8976, 0.8976, 0.8976, 0.8976, 0.8976, 0.8976,\n",
       "         0.8976, 0.8976, 0.8976, 0.8976, 0.8976, 0.8976, 0.8976, 0.8976, 0.8976,\n",
       "         0.8976, 0.8976, 0.8976, 0.8976, 0.8976, 0.8976, 0.8976, 0.8976, 0.8976,\n",
       "         0.8976, 0.8976, 0.8976, 0.8976, 0.8976, 0.8976, 0.8976, 0.8976, 0.8976,\n",
       "         0.8976, 0.8976, 0.8976, 0.8976, 0.8976, 0.8976, 0.8976, 0.8976, 0.8976,\n",
       "         0.8976, 0.8976, 0.8976, 0.8976, 0.8976, 0.8976, 0.8976, 0.8976, 0.8976,\n",
       "         0.8976, 0.8976]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor.fill_(torch.zeros(1, out_features), torch.pi * 2 / 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(over_sample_pred.T, over_sample_act[-1115:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(cnn_pred.T, cnn_act[-1115:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../hh_dataset/hh_npy/fib_hh102_mapping.json', 'r') as f:\n",
    "    activitiy_mapping = json.load(f)\n",
    "activitiy_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_act.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = multilabel_confusion_matrix(cnn_act[-1115:], cnn_pred.T)\n",
    "cf_dict = {}\n",
    "for act, cf in zip(activitiy_mapping.keys(), cf):\n",
    "    tp, fp, fn, tn = cf[0][0], cf[0][1], cf[1][0], cf[1][1]\n",
    "    cf_dict[act] = {'tp':tp, 'fp':fp,'fn':fn, 'tn':tn, 'support': np.count_nonzero(np.array(cnn_act[-1115:]).T[activitiy_mapping[act]]), 'total': np.count_nonzero(np.array(cnn_act).T[activitiy_mapping[act]])}\n",
    "pd.DataFrame(cf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result = pd.read_csv('./result_new/BiLSTM-(fib_hh102)_mean_with_weight/report.txt', sep='\\t')\n",
    "top_k = result.sort_values(by='positive\"F1').index[:7]\n",
    "top_k\n",
    "result.sort_values(by='positive\"F1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "loss = np.load('./result_new/BiLSTM-(fib_hh102)_mean_with_weight/losses.npy', allow_pickle=True)\n",
    "\n",
    "for i in range(len(loss)):\n",
    "    plt.plot(range(1, len(loss[i]) + 1), loss[i])\n",
    "plt.xlim([0, 20])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = {u'Label1':26, u'Label2': 17, u'Label3':30}\n",
    "count = np.count_nonzero(act.T == 1, axis=1)\n",
    "plt.bar(range(len(activitiy_mapping)-1), list(count), align='center')\n",
    "plt.xticks(range(len(activitiy_mapping)-1), list(activitiy_mapping.keys())[:-1], rotation=90)\n",
    "# # for python 2.x:\n",
    "# plt.bar(range(len(D)), D.values(), align='center')  # python 2.x\n",
    "# plt.xticks(range(len(D)), D.keys())  # in python 2.x\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('casas')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15 (default, Nov 24 2022, 21:12:53) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b07d098504fae4e94052a8c08597679d68db589eb60c25e2699a70e87af64b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
